[Meditation play]
[Hyper log log]

There are some algorithms which when I read, I am like, wow: I could have
never figured it out myself without some revelation or a flash of insight
from the heavens. Hyperloglog is one such algorithm.

In this video, my aim is to provide you that initial insight, and then lead
you through the how this amazing algorithm works.

So let's get started.

Suppose we have a sequence of some elements and we want to count how many some 
elements are there in the list. That's a straight forward problem. We just 
iterate through the list, incrementing a counter as we iterate.

Now we modify the problem slightly. Say, we want to count the number of distinct
elements in the sequence.

One way to achieve this would be to again, iterate throught the elements, but,
at each step, check back with the previous elements if the given element has
occurred earlier. This would lead to an essential a complete scan of the sequence
for each element, which is very bad
when dealing with a very large sequences consisting of millions or billions
of data points.

Another way to would be to store the set of distinct elements as we iterate
through the array. As we process a given element, we check in our store if
the element has been seen before. Using efficient data structures for the
element store, we can check for membership, in O(1) time. This ensures a
constant time processing per element, but the element store will store all
the distinct elements, and that too can get very large when there are millions
of distinct elemnets in the sequence.

Now, what if I tell you that I don't need the exact count of distinct elements,
but I am fine with an approximation. For instance, if 1,839,404 distinct users
accessed my website, I am fine with the algorithm returns me an answer of 1.8M,
or even 1.75M or 1.85M.

Given this relaxation, can you come up with an algorithm which takes constant
amount of time per element and uses constant amount of memory as well. That
sounds almost magical, but it is true.

----------------------

To motivate the algorithm, let's consider a totally different problem.

Suppose you start tossing a fair coin, and you keep a note of the longest
streak of heads that you encounter. For instance, in this sequence the longest 
streak is of length 5. Let's do another series of tosses. This time, the 
longest streak is of length 7.

In general, longer you toss a coin, larger the length of streak that you expect.
If you toss a coin 1000 times, you might have a streak of length of 10 or 15 maybe,
but if you toss a coin just 10 times, perhaps a straek of length 3 or 4 is the
best that you can expect.

So can we determine 
+++++++++++++++++++
Suppose you report back that the longest streak you encountered was of length 10.
Then it is reasonable for me to suppose that you tossed the coin for a pretty large
number of times than if you have reported back the longest streak length as 3. This
is because a streak of length 10 is much rarer occurrance. Getting 3 heads consecutively
is much more likely than getting 10 heads consecutively.

Can we formalize this intuition? The answer is: Yes. Though we won't go into 
rigorous maths, we can prove that if you toss a coin n times, you expect longest
streak to be of length around log n. You can read the paper linked in the description
if you are curious. Let's do some simulation and see how longest streak varies with
the number of tosses.

(http://www.csun.edu/~hcmth031/tlroh.pdf)

So, let's take a coin and start tossing.
Keep a note of the longest streak of heads that we encounter and plot that on a graph.
This is just one experiment. To find the expected longest streak, we repeat the experiment
multiple times, and take the average of the streak lengths for each number of tosses.

Only do simulation, and then tell final result.
------------
Same idea for cardinality.
n / 2 strings start with one 0
n / 4 strings start with two 0s
so 1 string should start with log_2^n 0s
log_2^n is max number of zeros.

So, if k is max number of zeros, 2^k is estimated cardinality.
After this idea, we need to do two thingsl
- Refine 2^k to make it accurate
- Take care of bad luck
